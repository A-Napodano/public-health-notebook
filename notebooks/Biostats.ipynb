{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d364e6f2",
   "metadata": {},
   "source": [
    "# üß† Python Library Reference for Public Health & Data Science\n",
    "\n",
    "## üìä Data Handling\n",
    "| Library     | Purpose                                      | Notes                                  |\n",
    "|-------------|----------------------------------------------|----------------------------------------|\n",
    "| pandas      | Data manipulation, cleaning, reshaping       | Ideal for tabular health datasets      |\n",
    "| NumPy       | Fast numerical operations, arrays            | Backbone for scientific computing      |\n",
    "\n",
    "## üìà Visualization\n",
    "| Library     | Purpose                                      | Notes                                  |\n",
    "|-------------|----------------------------------------------|----------------------------------------|\n",
    "| matplotlib  | Customizable plotting                        | Low-level control                      |\n",
    "| seaborn     | Statistical visualizations                   | Built on matplotlib, great defaults    |\n",
    "| plotly      | Interactive charts                           | Web-ready, dashboard-friendly          |\n",
    "\n",
    "## üß™ Statistical Analysis\n",
    "| Library       | Purpose                                    | Notes                                  |\n",
    "|---------------|--------------------------------------------|----------------------------------------|\n",
    "| SciPy         | Statistical tests, distributions           | Includes t-tests, ANOVA, etc.          |\n",
    "| statsmodels   | Regression, GLMs, time series              | Epidemiological modeling               |\n",
    "| pingouin      | Easy-to-use statistical tests              | Effect sizes, repeated measures        |\n",
    "\n",
    "## ‚è≥ Survival Analysis\n",
    "| Library     | Purpose                                      | Notes                                  |\n",
    "|-------------|----------------------------------------------|----------------------------------------|\n",
    "| lifelines   | Kaplan-Meier, Cox models                     | Visual survival curves                 |\n",
    "\n",
    "## ü§ñ Machine Learning\n",
    "| Library       | Purpose                                    | Notes                                  |\n",
    "|---------------|--------------------------------------------|----------------------------------------|\n",
    "| scikit-learn  | ML algorithms, preprocessing                | Classification, regression, clustering |\n",
    "| xgboost       | Gradient boosting                          | Fast, accurate, handles imbalance      |\n",
    "| lightgbm      | Gradient boosting                          | Faster training, lower memory usage    |\n",
    "\n",
    "## üåç Geospatial & Environmental\n",
    "| Library     | Purpose                                      | Notes                                  |\n",
    "|-------------|----------------------------------------------|----------------------------------------|\n",
    "| geopandas   | Spatial data manipulation                    | Built on pandas                        |\n",
    "| rasterio    | Raster data access                           | Environmental modeling                 |\n",
    "| shapely     | Geometric operations                         | Buffering, intersections               |\n",
    "\n",
    "## üß¨ Biomedical & Clinical\n",
    "| Library     | Purpose                                      | Notes                                  |\n",
    "|-------------|----------------------------------------------|----------------------------------------|\n",
    "| PyHealth    | Deep learning for EHR                        | Clinical prediction tasks              |\n",
    "| BioPython   | Genomic and biological sequence analysis     | DNA, protein, etc.                     |\n",
    "| pydicom     | Medical imaging (DICOM format)               | Radiology and imaging workflows        |\n",
    "\n",
    "## üîó Integration & APIs\n",
    "| Library     | Purpose                                      | Notes                                  |\n",
    "|-------------|----------------------------------------------|----------------------------------------|\n",
    "| fhirclient  | HL7 FHIR API access                          | Clinical data interoperability         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8fd10",
   "metadata": {},
   "source": [
    "# Basic Data Analysis and Visualization in Python\n",
    "\n",
    "This guide provides a walkthrough of common exploratory data analysis tasks using Python, primarily with the **pandas**, **seaborn**, and **matplotlib** libraries.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup and Placing Data into a DataFrame\n",
    "\n",
    "The first step in any analysis is to import the necessary libraries and load your data into a pandas DataFrame. A DataFrame is a 2-dimensional labeled data structure, like a spreadsheet.\n",
    "\n",
    "### Code\n",
    "```python\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a sample list of data\n",
    "data = [25, 41, 33, 29, 45, 25, 38, 33, 40, 22, 51, 48, 35, 33, 42]\n",
    "\n",
    "# Place the data into a pandas DataFrame\n",
    "df = pd.DataFrame({'score': data})\n",
    "print(\"DataFrame created:\")\n",
    "print(df.head())\n",
    "```\n",
    "Instead of typing or pasting in data as a list, you can import a .csv file to work with. For the simplest import, ensure the file is saved in the same folder as your Python script or notebook. If the file is not in the same directory, you will need to provide the full file path to the pd.read_csv() function.\n",
    "\n",
    "### Code\n",
    "```python\n",
    "# Read the .csv file and create a DataFrame\n",
    "df = pd.read_csv('sample_data.csv')\n",
    "print(\"DataFrame created:\")\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Sorting Data\n",
    "\n",
    "You can easily sort the data in a DataFrame from the smallest to the largest value.\n",
    "\n",
    "### Code\n",
    "```python\n",
    "# Sort the DataFrame by the 'score' column\n",
    "df_sorted = df.sort_values(by='score')\n",
    "\n",
    "print(\"Sorted DataFrame:\")\n",
    "print(df_sorted.head())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Frequency Distribution Tables\n",
    "\n",
    "A frequency distribution shows how often different values occur in a dataset. We can create bins to group continuous data and calculate the frequency, relative frequency, and cumulative frequency.\n",
    "\n",
    "### Code\n",
    "```python\n",
    "# 0. (optional) Determine bin size using Sturge's rule. Alternatively, import my_stats_tools as mst.\n",
    "def sturges_step(data):\n",
    "    n = len(data)\n",
    "    k = math.ceil(np.log2(n) + 1) # number of bins\n",
    "    step = (np.max(data) - np.min(data)) / k\n",
    "    return step\n",
    "\n",
    "print(f\"Suggested step size: {step}\")\n",
    "\n",
    "# 1. Create bins for the data\n",
    "bins = np.arange(20, 60, 5) # Bins of size 5, from 20 up to (but not including) 60\n",
    "\n",
    "# 2. Group data into bins\n",
    "df['binned'] = pd.cut(df['score'], bins=bins, right=False)\n",
    "\n",
    "# 3. Calculate frequencies\n",
    "freq = df['binned'].value_counts().sort_index()\n",
    "rel_freq = df['binned'].value_counts(normalize=True).sort_index()\n",
    "cum_freq = rel_freq.cumsum() # Cumulative sum of the relative frequency\n",
    "\n",
    "# 4. Combine into a single table\n",
    "dist_table = pd.DataFrame({\n",
    "    'Frequency': freq,\n",
    "    'Relative Frequency': rel_freq,\n",
    "    'Cumulative Frequency': cum_freq\n",
    "})\n",
    "\n",
    "print(\"Frequency Distribution Table:\")\n",
    "print(dist_table)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Measures of Central Tendency\n",
    "\n",
    "Measures of central tendency describe the center of a dataset.\n",
    "\n",
    "### Code\n",
    "```python\n",
    "# Calculate Mean\n",
    "mean_val = df['score'].mean()\n",
    "\n",
    "# Calculate Median\n",
    "median_val = df['score'].median()\n",
    "\n",
    "# Calculate Mode\n",
    "mode_val = df['score'].mode()\n",
    "\n",
    "print(f\"Mean: {mean_val:.2f}\")\n",
    "print(f\"Median: {median_val:.2f}\")\n",
    "print(f\"Mode: {mode_val}\")\n",
    "```\n",
    "You can also get the mean and median from the `.describe()` method:\n",
    "```python\n",
    "print(df['score'].describe())\n",
    "```\n",
    "---\n",
    "\n",
    "## 5. Measures of Dispersion\n",
    "\n",
    "Measures of dispersion (or variability) describe the spread of the data.\n",
    "\n",
    "### Code\n",
    "```python\n",
    "# Calculate Range\n",
    "range_val = df['score'].max() - df['score'].min()\n",
    "\n",
    "# Calculate Variance\n",
    "var_val = df['score'].var()\n",
    "\n",
    "# Calculate Standard Deviation\n",
    "std_val = df['score'].std()\n",
    "\n",
    "# Calculate Coefficient of Variation\n",
    "CV_val = std_val / mean_val\n",
    "\n",
    "# Calculate Interquartile Range (IQR)\n",
    "q1 = df['score'].quantile(0.25) # Be advised that these are interpolated values\n",
    "q3 = df['score'].quantile(0.75) # Be advised that these are interpolated values\n",
    "iqr_val = q3 - q1\n",
    "\n",
    "print(f\"Range: {range_val}\")\n",
    "print(f\"Variance: {var_val:.2f}\")\n",
    "print(f\"Standard Deviation: {std_val:.2f}\")\n",
    "print(f\"Coefficient of Variation: {CV_val:.2f}\")\n",
    "print(f\"First Quartile Q1: {q1}\")\n",
    "print(f\"Third Quartile Q3: {q3}\")\n",
    "print(f\"Interquartile Range (IQR): {iqr_val}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Skewness, Kurtosis, and Modality\n",
    "\n",
    "These measures describe the shape of the data's distribution.\n",
    "\n",
    "* **Skewness**: Measures the asymmetry of the distribution.\n",
    "* **Kurtosis**: Measures the \"tailedness\" of the distribution.\n",
    "* **Modality**: Describes the number of peaks in the distribution (unimodal, bimodal, etc.). This is observed visually from a histogram.\n",
    "\n",
    "### Code\n",
    "```python\n",
    "# Calculate Skewness\n",
    "skew_val = df['score'].skew()\n",
    "\n",
    "# Calculate Kurtosis\n",
    "kurt_val = df['score'].kurt()\n",
    "\n",
    "print(f\"Skewness: {skew_val:.2f}\")\n",
    "print(f\"Kurtosis: {kurt_val:.2f}\")\n",
    "print(\"Modality: Observe the number of peaks in the histogram below.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Data Visualization\n",
    "\n",
    "Visualizing data is crucial for understanding its characteristics.\n",
    "\n",
    "### Stem-and-Leaf Plot\n",
    "There is no built-in function in pandas/seaborn, so we create our own. Alternatively, import my_stats_tools as mst.\n",
    "\n",
    "```python\n",
    "def create_stem_and_leaf(data_list, title=\"Stem-and-Leaf Display\"):\n",
    "    print(title)\n",
    "    print(\"-\" * len(title))\n",
    "    if not data_list:\n",
    "        print(\"Data list is empty.\"); return\n",
    "    stem_leaf = {}; data_list.sort()\n",
    "    for num in data_list:\n",
    "        stem, leaf = num // 10, num % 10\n",
    "        if stem not in stem_leaf: stem_leaf[stem] = []\n",
    "        stem_leaf[stem].append(leaf)\n",
    "    for stem, leaves in sorted(stem_leaf.items()):\n",
    "        print(f\" {stem} | {' '.join(map(str, leaves))}\")\n",
    "\n",
    "create_stem_and_leaf(df['score'].tolist())\n",
    "```\n",
    "\n",
    "### Histogram\n",
    "A histogram shows the frequency distribution as bars.\n",
    "\n",
    "\n",
    "```python\n",
    "sns.histplot(data=df, x='score', bins=bins, kde=True)\n",
    "plt.title('Histogram of Scores')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Frequency Polygon\n",
    "A line graph connecting the midpoints of the histogram bars.\n",
    "\n",
    "\n",
    "```python\n",
    "# Calculate midpoints and frequencies for the polygon\n",
    "frequencies = dist_table['Frequency'].values\n",
    "frequencies_anchored = np.concatenate(([0], frequencies, [0]))\n",
    "midpoints = np.array(bins[:-1]) + 2.5\n",
    "midpoints_anchored = np.concatenate(([midpoints[0] - 5], midpoints, [midpoints[-1] + 5]))\n",
    "\n",
    "# Plot\n",
    "sns.histplot(data=df, x='score', bins=bins, color='lightblue', alpha=0.5)\n",
    "plt.plot(midpoints_anchored, frequencies_anchored, marker='o', color='red', label='Frequency Polygon')\n",
    "plt.title('Frequency Polygon of Scores')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Box-and-Whisker Plot\n",
    "A box plot summarizes the five-number summary: minimum, Q1, median, Q3, and maximum. It's excellent for spotting outliers.\n",
    "\n",
    "```python\n",
    "sns.boxplot(data=df, y='score')\n",
    "plt.title('Box-and-Whisker Plot of Scores')\n",
    "plt.show()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
