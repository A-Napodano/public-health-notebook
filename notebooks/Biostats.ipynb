{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbe8fd10",
   "metadata": {},
   "source": [
    "# Basic Data Analysis and Visualization in Python\n",
    "\n",
    "This guide provides a walkthrough of common exploratory data analysis tasks using Python, primarily with the **pandas**, **seaborn**, and **matplotlib** libraries.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup and Placing Data into a DataFrame\n",
    "\n",
    "The first step in any analysis is to import the necessary libraries and load your data into a pandas DataFrame. A DataFrame is a 2-dimensional labeled data structure, like a spreadsheet.\n",
    "\n",
    "### Code\n",
    "```python\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a sample list of data\n",
    "data = [25, 41, 33, 29, 45, 25, 38, 33, 40, 22, 51, 48, 35, 33, 42]\n",
    "\n",
    "# Place the data into a pandas DataFrame\n",
    "df = pd.DataFrame({'score': data})\n",
    "print(\"DataFrame created:\")\n",
    "print(df.head())\n",
    "```\n",
    "Instead of typing or pasting in data as a list, you can import a .csv file to work with. For the simplest import, ensure the file is saved in the same folder as your Python script or notebook. If the file is not in the same directory, you will need to provide the full file path to the pd.read_csv() function.\n",
    "\n",
    "### Code\n",
    "```python\n",
    "# Read the .csv file and create a DataFrame\n",
    "df = pd.read_csv('sample_data.csv')\n",
    "print(\"DataFrame created:\")\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Sorting Data\n",
    "\n",
    "You can easily sort the data in a DataFrame from the smallest to the largest value.\n",
    "\n",
    "### Code\n",
    "```python\n",
    "# Sort the DataFrame by the 'score' column\n",
    "df_sorted = df.sort_values(by='score')\n",
    "\n",
    "print(\"Sorted DataFrame:\")\n",
    "print(df_sorted.head())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Frequency Distribution Tables\n",
    "\n",
    "A frequency distribution shows how often different values occur in a dataset. We can create bins to group continuous data and calculate the frequency, relative frequency, and cumulative frequency.\n",
    "\n",
    "### Code\n",
    "```python\n",
    "# 0. (optional) Determine bin size using Sturge's rule. Alternatively, import my_stats_tools as mst.\n",
    "def sturges_step(data):\n",
    "    n = len(data)\n",
    "    k = math.ceil(np.log2(n) + 1) # number of bins\n",
    "    step = (np.max(data) - np.min(data)) / k\n",
    "    return step\n",
    "\n",
    "print(f\"Suggested step size: {step}\")\n",
    "\n",
    "# 1. Create bins for the data\n",
    "bins = np.arange(20, 60, 5) # Bins of size 5, from 20 up to (but not including) 60\n",
    "\n",
    "# 2. Group data into bins\n",
    "df['binned'] = pd.cut(df['score'], bins=bins, right=False)\n",
    "\n",
    "# 3. Calculate frequencies\n",
    "freq = df['binned'].value_counts().sort_index()\n",
    "rel_freq = df['binned'].value_counts(normalize=True).sort_index()\n",
    "cum_freq = rel_freq.cumsum() # Cumulative sum of the relative frequency\n",
    "\n",
    "# 4. Combine into a single table\n",
    "dist_table = pd.DataFrame({\n",
    "    'Frequency': freq,\n",
    "    'Relative Frequency': rel_freq,\n",
    "    'Cumulative Frequency': cum_freq\n",
    "})\n",
    "\n",
    "print(\"Frequency Distribution Table:\")\n",
    "print(dist_table)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Measures of Central Tendency\n",
    "\n",
    "Measures of central tendency describe the center of a dataset.\n",
    "\n",
    "### Code\n",
    "```python\n",
    "# Calculate Mean\n",
    "mean_val = df['score'].mean()\n",
    "\n",
    "# Calculate Median\n",
    "median_val = df['score'].median()\n",
    "\n",
    "# Calculate Mode\n",
    "mode_val = df['score'].mode()\n",
    "\n",
    "print(f\"Mean: {mean_val:.2f}\")\n",
    "print(f\"Median: {median_val:.2f}\")\n",
    "print(f\"Mode: {mode_val}\")\n",
    "```\n",
    "You can also get the mean and median from the `.describe()` method:\n",
    "```python\n",
    "print(df['score'].describe())\n",
    "```\n",
    "---\n",
    "\n",
    "## 5. Measures of Dispersion\n",
    "\n",
    "Measures of dispersion (or variability) describe the spread of the data.\n",
    "\n",
    "### Code\n",
    "```python\n",
    "# Calculate Range\n",
    "range_val = df['score'].max() - df['score'].min()\n",
    "\n",
    "# Calculate Variance\n",
    "var_val = df['score'].var()\n",
    "\n",
    "# Calculate Standard Deviation\n",
    "std_val = df['score'].std()\n",
    "\n",
    "# Calculate Coefficient of Variation\n",
    "CV_val = std_val / mean_val\n",
    "\n",
    "# Calculate Interquartile Range (IQR)\n",
    "q1 = df['score'].quantile(0.25) # Be advised that these are interpolated values\n",
    "q3 = df['score'].quantile(0.75) # Be advised that these are interpolated values\n",
    "iqr_val = q3 - q1\n",
    "\n",
    "print(f\"Range: {range_val}\")\n",
    "print(f\"Variance: {var_val:.2f}\")\n",
    "print(f\"Standard Deviation: {std_val:.2f}\")\n",
    "print(f\"Coefficient of Variation: {CV_val:.2f}\")\n",
    "print(f\"First Quartile Q1: {q1}\")\n",
    "print(f\"Third Quartile Q3: {q3}\")\n",
    "print(f\"Interquartile Range (IQR): {iqr_val}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Skewness, Kurtosis, and Modality\n",
    "\n",
    "These measures describe the shape of the data's distribution.\n",
    "\n",
    "* **Skewness**: Measures the asymmetry of the distribution.\n",
    "* **Kurtosis**: Measures the \"tailedness\" of the distribution.\n",
    "* **Modality**: Describes the number of peaks in the distribution (unimodal, bimodal, etc.). This is observed visually from a histogram.\n",
    "\n",
    "### Code\n",
    "```python\n",
    "# Calculate Skewness\n",
    "skew_val = df['score'].skew()\n",
    "\n",
    "# Calculate Kurtosis\n",
    "kurt_val = df['score'].kurt()\n",
    "\n",
    "print(f\"Skewness: {skew_val:.2f}\")\n",
    "print(f\"Kurtosis: {kurt_val:.2f}\")\n",
    "print(\"Modality: Observe the number of peaks in the histogram below.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Data Visualization\n",
    "\n",
    "Visualizing data is crucial for understanding its characteristics.\n",
    "\n",
    "### Stem-and-Leaf Plot\n",
    "There is no built-in function in pandas/seaborn, so we create our own. Alternatively, import my_stats_tools as mst.\n",
    "\n",
    "```python\n",
    "def create_stem_and_leaf(data_list, title=\"Stem-and-Leaf Display\"):\n",
    "    print(title)\n",
    "    print(\"-\" * len(title))\n",
    "    if not data_list:\n",
    "        print(\"Data list is empty.\"); return\n",
    "    stem_leaf = {}; data_list.sort()\n",
    "    for num in data_list:\n",
    "        stem, leaf = num // 10, num % 10\n",
    "        if stem not in stem_leaf: stem_leaf[stem] = []\n",
    "        stem_leaf[stem].append(leaf)\n",
    "    for stem, leaves in sorted(stem_leaf.items()):\n",
    "        print(f\" {stem} | {' '.join(map(str, leaves))}\")\n",
    "\n",
    "create_stem_and_leaf(df['score'].tolist())\n",
    "```\n",
    "\n",
    "### Histogram\n",
    "A histogram shows the frequency distribution as bars.\n",
    "\n",
    "\n",
    "```python\n",
    "sns.histplot(data=df, x='score', bins=bins, kde=True)\n",
    "plt.title('Histogram of Scores')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Frequency Polygon\n",
    "A line graph connecting the midpoints of the histogram bars.\n",
    "\n",
    "\n",
    "```python\n",
    "# Calculate midpoints and frequencies for the polygon\n",
    "frequencies = dist_table['Frequency'].values\n",
    "frequencies_anchored = np.concatenate(([0], frequencies, [0]))\n",
    "midpoints = np.array(bins[:-1]) + 2.5\n",
    "midpoints_anchored = np.concatenate(([midpoints[0] - 5], midpoints, [midpoints[-1] + 5]))\n",
    "\n",
    "# Plot\n",
    "sns.histplot(data=df, x='score', bins=bins, color='lightblue', alpha=0.5)\n",
    "plt.plot(midpoints_anchored, frequencies_anchored, marker='o', color='red', label='Frequency Polygon')\n",
    "plt.title('Frequency Polygon of Scores')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Box-and-Whisker Plot\n",
    "A box plot summarizes the five-number summary: minimum, Q1, median, Q3, and maximum. It's excellent for spotting outliers.\n",
    "\n",
    "```python\n",
    "sns.boxplot(data=df, y='score')\n",
    "plt.title('Box-and-Whisker Plot of Scores')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Contingency Tables and Probabilities\n",
    "Contingency tables can be used to calculate joint, marginal, and conditonal probabilities from a dataset containing at least two categorical variables. In this example, our DataFrame is `df` and we will use `'variable_1'` (for rows) and `'variable_2'` (for columns). `margins=True` provides row and column totals, which are very helpful.\n",
    "\n",
    "### Code\n",
    "```python\n",
    "# Create a table of counts with row and column totals\n",
    "contingency_table = pd.crosstab(df['variable_1'], df['variable_2'], margins=True, margins_name='Total')\n",
    "\n",
    "print(\"Contingency Table (Counts):\")\n",
    "print(contingency_table)\n",
    "```\n",
    "\n",
    "**Joint probability** is the probability of two events occurring at the same time: `P(A and B)`. To calculate this, divide every count in the table by the grand total (`normalize='all'`).\n",
    "\n",
    "```python\n",
    "# Create a table of joint probabilities\n",
    "joint_prob_table = pd.crosstab(df['variable_1'], df['variable_2'], normalize='all')\n",
    "\n",
    "print(\"Joint Probability Table:\")\n",
    "print(joint_prob_table)\n",
    "```\n",
    "\n",
    "**Marginal probability** is the probability of a single event occurring, regardless of other variables: `P(A)`. These values are found in the margins of the table. If `margins=True` and `normalize='all'`, the margin values will be normalized (divided by the total).\n",
    "\n",
    "```python\n",
    "# Create a table of marginal probabilities\n",
    "marginal_prob_table = pd.crosstab(df['variable_1'], df['variable_2'], margins=True, normalize='all')\n",
    "\n",
    "print(\"Joint and Marginal Probability Table:\")\n",
    "print(marginal_prob_table)\n",
    "```\n",
    "\n",
    "**Conditional probability** is the probability of an event occurring **given** that another event has already occurred: `P(A | B)` asks \"What is the probability of A given that B has occured?\" In other words, you are conditioning on the columns (assuming A is the rows or 'variable_1', and B is the columns or 'variable_2').\n",
    "\n",
    "```python\n",
    "# Condition on the columns i.e. P(A | B)\n",
    "conditional_prob_columns = pd.crosstab(df['variable_1'], df['variable_2'], normalize='columns')\n",
    "\n",
    "print(\"Conditional Probabilities: P (variable_1 | variable_2):\")\n",
    "print(conditional_prob_columns)\n",
    "\n",
    "# Condition on the rows i.e. P(B | A)\n",
    "conditional_prob_rows = pd.crosstab(df['variable_1'], df['variable_2'], normalize='index')\n",
    "\n",
    "print(\"Conditional Probabilities: P (variable_2 | variable_1):\")\n",
    "print(conditional_prob_rows)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biostats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
