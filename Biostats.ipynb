{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbe8fd10",
   "metadata": {},
   "source": [
    "# Basic Data Analysis and Visualization in Python\n",
    "\n",
    "This guide provides a walkthrough of common exploratory data analysis tasks using Python, primarily with the **pandas**, **seaborn**, and **matplotlib** libraries.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup and Placing Data into a DataFrame\n",
    "\n",
    "The first step in any analysis is to import the necessary libraries and load your data into a pandas DataFrame. A DataFrame is a 2-dimensional labeled data structure, like a spreadsheet.\n",
    "\n",
    "### Code\n",
    "```python\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a sample list of data\n",
    "data = [25, 41, 33, 29, 45, 25, 38, 33, 40, 22, 51, 48, 35, 33, 42]\n",
    "\n",
    "# Place the data into a pandas DataFrame\n",
    "df = pd.DataFrame({'score': data})\n",
    "print(\"DataFrame created:\")\n",
    "print(df.head())\n",
    "```\n",
    "Instead of typing or pasting in data as a list, you can import a .csv file to work with. For the simplest import, ensure the file is saved in the same folder as your Python script or notebook. If the file is not in the same directory, you will need to provide the full file path to the pd.read_csv() function.\n",
    "\n",
    "### Code\n",
    "```python\n",
    "# Read the .csv file and create a DataFrame\n",
    "df = pd.read_csv('sample_data.csv')\n",
    "print(\"DataFrame created:\")\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Sorting Data\n",
    "\n",
    "You can easily sort the data in a DataFrame from the smallest to the largest value.\n",
    "\n",
    "### Code\n",
    "```python\n",
    "# Sort the DataFrame by the 'score' column\n",
    "df_sorted = df.sort_values(by='score')\n",
    "\n",
    "print(\"Sorted DataFrame:\")\n",
    "print(df_sorted.head())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Frequency Distribution Tables\n",
    "\n",
    "A frequency distribution shows how often different values occur in a dataset. We can create bins to group continuous data and calculate the frequency, relative frequency, and cumulative frequency.\n",
    "\n",
    "### Code\n",
    "```python\n",
    "# 0. (optional) Determine bin size using Sturge's rule. Alternatively, import my_stats_tools as mst.\n",
    "def sturges_step(data):\n",
    "    n = len(data)\n",
    "    k = math.ceil(np.log2(n) + 1) # number of bins\n",
    "    step = (np.max(data) - np.min(data)) / k\n",
    "    return step\n",
    "\n",
    "print(f\"Suggested step size: {step}\")\n",
    "\n",
    "# 1. Create bins for the data\n",
    "bins = np.arange(20, 60, 5) # Bins of size 5, from 20 up to (but not including) 60\n",
    "\n",
    "# 2. Group data into bins\n",
    "df['binned'] = pd.cut(df['score'], bins=bins, right=False)\n",
    "\n",
    "# 3. Calculate frequencies\n",
    "freq = df['binned'].value_counts().sort_index()\n",
    "rel_freq = df['binned'].value_counts(normalize=True).sort_index()\n",
    "cum_freq = rel_freq.cumsum() # Cumulative sum of the relative frequency\n",
    "\n",
    "# 4. Combine into a single table\n",
    "dist_table = pd.DataFrame({\n",
    "    'Frequency': freq,\n",
    "    'Relative Frequency': rel_freq,\n",
    "    'Cumulative Frequency': cum_freq\n",
    "})\n",
    "\n",
    "print(\"Frequency Distribution Table:\")\n",
    "print(dist_table)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Measures of Central Tendency\n",
    "\n",
    "Measures of central tendency describe the center of a dataset.\n",
    "\n",
    "### Code\n",
    "```python\n",
    "# Calculate Mean\n",
    "mean_val = df['score'].mean()\n",
    "\n",
    "# Calculate Median\n",
    "median_val = df['score'].median()\n",
    "\n",
    "# Calculate Mode\n",
    "mode_val = df['score'].mode()\n",
    "\n",
    "print(f\"Mean: {mean_val:.2f}\")\n",
    "print(f\"Median: {median_val:.2f}\")\n",
    "print(f\"Mode: {mode_val}\")\n",
    "```\n",
    "You can also get the mean and median from the `.describe()` method:\n",
    "```python\n",
    "print(df['score'].describe())\n",
    "```\n",
    "---\n",
    "\n",
    "## 5. Measures of Dispersion\n",
    "\n",
    "Measures of dispersion (or variability) describe the spread of the data.\n",
    "\n",
    "### Code\n",
    "```python\n",
    "# Calculate Range\n",
    "range_val = df['score'].max() - df['score'].min()\n",
    "\n",
    "# Calculate Variance\n",
    "var_val = df['score'].var()\n",
    "\n",
    "# Calculate Standard Deviation\n",
    "std_val = df['score'].std()\n",
    "\n",
    "# Calculate Coefficient of Variation\n",
    "CV_val = std_val / mean_val\n",
    "\n",
    "# Calculate Interquartile Range (IQR)\n",
    "q1 = df['score'].quantile(0.25) # Be advised that these are interpolated values\n",
    "q3 = df['score'].quantile(0.75) # Be advised that these are interpolated values\n",
    "iqr_val = q3 - q1\n",
    "\n",
    "print(f\"Range: {range_val}\")\n",
    "print(f\"Variance: {var_val:.2f}\")\n",
    "print(f\"Standard Deviation: {std_val:.2f}\")\n",
    "print(f\"Coefficient of Variation: {CV_val:.2f}\")\n",
    "print(f\"First Quartile Q1: {q1}\")\n",
    "print(f\"Third Quartile Q3: {q3}\")\n",
    "print(f\"Interquartile Range (IQR): {iqr_val}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Skewness, Kurtosis, and Modality\n",
    "\n",
    "These measures describe the shape of the data's distribution.\n",
    "\n",
    "* **Skewness**: Measures the asymmetry of the distribution.\n",
    "* **Kurtosis**: Measures the \"tailedness\" of the distribution.\n",
    "* **Modality**: Describes the number of peaks in the distribution (unimodal, bimodal, etc.). This is observed visually from a histogram.\n",
    "\n",
    "### Code\n",
    "```python\n",
    "# Calculate Skewness\n",
    "skew_val = df['score'].skew()\n",
    "\n",
    "# Calculate Kurtosis\n",
    "kurt_val = df['score'].kurt()\n",
    "\n",
    "print(f\"Skewness: {skew_val:.2f}\")\n",
    "print(f\"Kurtosis: {kurt_val:.2f}\")\n",
    "print(\"Modality: Observe the number of peaks in the histogram below.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Data Visualization\n",
    "\n",
    "Visualizing data is crucial for understanding its characteristics.\n",
    "\n",
    "### Stem-and-Leaf Plot\n",
    "There is no built-in function in pandas/seaborn, so we create our own. Alternatively, import my_stats_tools as mst.\n",
    "\n",
    "```python\n",
    "def create_stem_and_leaf(data_list, title=\"Stem-and-Leaf Display\"):\n",
    "    print(title)\n",
    "    print(\"-\" * len(title))\n",
    "    if not data_list:\n",
    "        print(\"Data list is empty.\"); return\n",
    "    stem_leaf = {}; data_list.sort()\n",
    "    for num in data_list:\n",
    "        stem, leaf = num // 10, num % 10\n",
    "        if stem not in stem_leaf: stem_leaf[stem] = []\n",
    "        stem_leaf[stem].append(leaf)\n",
    "    for stem, leaves in sorted(stem_leaf.items()):\n",
    "        print(f\" {stem} | {' '.join(map(str, leaves))}\")\n",
    "\n",
    "create_stem_and_leaf(df['score'].tolist())\n",
    "```\n",
    "\n",
    "### Histogram\n",
    "A histogram shows the frequency distribution as bars.\n",
    "\n",
    "\n",
    "```python\n",
    "sns.histplot(data=df, x='score', bins=bins, kde=True)\n",
    "plt.title('Histogram of Scores')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Frequency Polygon\n",
    "A line graph connecting the midpoints of the histogram bars.\n",
    "\n",
    "\n",
    "```python\n",
    "# Calculate midpoints and frequencies for the polygon\n",
    "frequencies = dist_table['Frequency'].values\n",
    "frequencies_anchored = np.concatenate(([0], frequencies, [0]))\n",
    "midpoints = np.array(bins[:-1]) + 2.5\n",
    "midpoints_anchored = np.concatenate(([midpoints[0] - 5], midpoints, [midpoints[-1] + 5]))\n",
    "\n",
    "# Plot\n",
    "sns.histplot(data=df, x='score', bins=bins, color='lightblue', alpha=0.5)\n",
    "plt.plot(midpoints_anchored, frequencies_anchored, marker='o', color='red', label='Frequency Polygon')\n",
    "plt.title('Frequency Polygon of Scores')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Box-and-Whisker Plot\n",
    "A box plot summarizes the five-number summary: minimum, Q1, median, Q3, and maximum. It's excellent for spotting outliers.\n",
    "\n",
    "```python\n",
    "sns.boxplot(data=df, y='score')\n",
    "plt.title('Box-and-Whisker Plot of Scores')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Contingency Tables and Probabilities\n",
    "\n",
    "Contingency tables can be used to calculate joint, marginal, and conditonal probabilities from a dataset containing at least two categorical variables. In this example, our DataFrame is `df` and we will use `'variable_1'` (for rows) and `'variable_2'` (for columns). `margins=True` provides row and column totals, which are very helpful.\n",
    "\n",
    "### Code\n",
    "```python\n",
    "# Create a table of counts with row and column totals\n",
    "contingency_table = pd.crosstab(df['variable_1'], df['variable_2'], margins=True, margins_name='Total')\n",
    "\n",
    "print(\"Contingency Table (Counts):\")\n",
    "print(contingency_table)\n",
    "```\n",
    "\n",
    "**Joint probability** is the probability of two events occurring at the same time: `P(A and B)`. To calculate this, divide every count in the table by the grand total (`normalize='all'`).\n",
    "\n",
    "```python\n",
    "# Create a table of joint probabilities\n",
    "joint_prob_table = pd.crosstab(df['variable_1'], df['variable_2'], normalize='all')\n",
    "\n",
    "print(\"Joint Probability Table:\")\n",
    "print(joint_prob_table)\n",
    "```\n",
    "\n",
    "**Marginal probability** is the probability of a single event occurring, regardless of other variables: `P(A)`. These values are found in the margins of the table. If `margins=True` and `normalize='all'`, the margin values will be normalized (divided by the total).\n",
    "\n",
    "```python\n",
    "# Create a table of marginal probabilities\n",
    "marginal_prob_table = pd.crosstab(df['variable_1'], df['variable_2'], margins=True, normalize='all')\n",
    "\n",
    "print(\"Joint and Marginal Probability Table:\")\n",
    "print(marginal_prob_table)\n",
    "```\n",
    "\n",
    "**Conditional probability** is the probability of an event occurring **given** that another event has already occurred: `P(A | B)` asks \"What is the probability of A given that B has occured?\" In other words, you are conditioning on the columns (assuming A is the rows or 'variable_1', and B is the columns or 'variable_2').\n",
    "\n",
    "```python\n",
    "# Condition on the columns i.e. P(A | B)\n",
    "conditional_prob_columns = pd.crosstab(df['variable_1'], df['variable_2'], normalize='columns')\n",
    "\n",
    "print(\"Conditional Probabilities: P (variable_1 | variable_2):\")\n",
    "print(conditional_prob_columns)\n",
    "\n",
    "# Condition on the rows i.e. P(B | A)\n",
    "conditional_prob_rows = pd.crosstab(df['variable_1'], df['variable_2'], normalize='index')\n",
    "\n",
    "print(\"Conditional Probabilities: P (variable_2 | variable_1):\")\n",
    "print(conditional_prob_rows)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Probability Distributions\n",
    "\n",
    "This section provides general instructions for calculating probabilities using three fundamental distributions from the `scipy.stats` library: Binomial, Poisson, and Normal. The general workflow involves:\n",
    "1. Import the specific distribution object from `scipy.stats`.\n",
    "2. Identify the parameters that define the distribution's shape and location.\n",
    "3. Use the object's methods to calculate the desired probabilities.\n",
    "\n",
    "### Key Functions\n",
    "* **`.pmf(k, ...)` (Probability Mass Function)**: For **discrete** distributions (Binomial, Poisson). Calculates the probability of observing *exactly* `k` successes or events.\n",
    "* **`.cdf(k, ...)` (Cumulative Distribution Function)**: For any distribution. Calculates the probability of observing a value *less than or equal to* `k`.\n",
    "* **`.sf(k, ...)` (Survival Function)**: For any distribution. Calculates the probability of observing a value *greater than* `k`. This is equivalent to `1 - .cdf(k)`.\n",
    "\n",
    "### Binomial Distribution\n",
    "Use this for modeling the number of \"successes\" `k` in a fixed number of independent trials `n` where each trial has only two outcomes and a known probability of success `p`.\n",
    "\n",
    "```python\n",
    "from scipy.stats import binom\n",
    "\n",
    "# Define parameters\n",
    "n = number_of_trials\n",
    "p = probability_of_success\n",
    "\n",
    "# Calculations\n",
    "#P(X = k)\n",
    "prob = binom.pmf(k, n, p)\n",
    "#P(X <= k)\n",
    "prob = binom.cdf(k, n, p)\n",
    "#P(X > k)\n",
    "prob = binom.sf(k, n, p)\n",
    "#P(X >= k)\n",
    "prob = binom.sf(k - 1, n, p)\n",
    "```\n",
    "\n",
    "### Poisson Distribution\n",
    "Use this for modeling the number of times an event occurs `k` over a fixed interval of time or space, given a known average rate `mu` or `λ`.\n",
    "\n",
    "```python\n",
    "from scipy.stats import poisson\n",
    "\n",
    "# Define parameters\n",
    "mu = average_rate_of_event\n",
    "\n",
    "# Calculations\n",
    "#P(X = k)\n",
    "prob = poisson.pmf(k, mu)\n",
    "#P(X <= k)\n",
    "prob = poisson.cdf(k, mu)\n",
    "#P(X > k)\n",
    "prob = poisson.sf(k, mu)\n",
    "#P(X >= k)\n",
    "prob = poisson.sf(k - 1, mu)\n",
    "```\n",
    "\n",
    "### Normal Distribution\n",
    "Use this for modeling **continuous** data that follows a bell-shaped curve. We always calculate probabilities over a range. The required parameters are:\n",
    "* `loc` (or `μ`): A float representing the mean of the distribution.\n",
    "* `scale` (or `σ`): A float representing the standard deviation of the distribution.\n",
    "* `x`: A float representing the point of interest.\n",
    "\n",
    "```python\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Define parameters\n",
    "loc = mean_of_distribution\n",
    "scale = standard_deviation_of_distribution\n",
    "\n",
    "# Calculations\n",
    "#P(X <= x)\n",
    "prob = norm.cdf(x, loc, scale)\n",
    "#P(X > x)\n",
    "prob = norm.sf(x, loc, scale)\n",
    "#P(x1 < X <= x2) which is calculated as P(X <= x2) - P(X <= x1)\n",
    "prob = norm.cdf(x2, loc, scale) - norm.cdf(x1, loc, scale)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Sampling Distributions\n",
    "\n",
    "There are two ways to draw a sample: *with replacement* and *without replacement*. In sampling without replacement, the unit is not returned to the population and cannot be selected again (e.g. drawing names from a hat). Both type of sampling can be performed using pandas' `.sample()` method.\n",
    "\n",
    "### Code\n",
    "```python\n",
    "import pandas as pd\n",
    "population_df = pd.DataFrame({'patient_id': range(1,21)}) # A population of 20 patients\n",
    "\n",
    "# Sampling WITHOUT replacement\n",
    "sample_without = population_df.sample(n=5, replace=False)\n",
    "print(\"Sample without replacement:\\n\", sample_without)\n",
    "\n",
    "# Sampling WITH replacement\n",
    "sample_with = population_df.sample(n=5, replace=True)\n",
    "print(\"\\nSample with replacement:\\n\", sample_with)\n",
    "```\n",
    "\n",
    "A **sampling distribution** is the probability distribution of a statistic (like a sample mean) obtained from all possible samples of the same size from the same population. Here is an example of what this would look like in practice:\n",
    "\n",
    "### Code\n",
    "```python\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Create a hypothetical population (e.g., hospital stay durations in days)\n",
    "# We'll make it skewed to the right to show the power of the Central Limit Theorem.\n",
    "np.random.seed(42)\n",
    "population = np.random.gamma(shape=2, scale=2, size=10000)\n",
    "\n",
    "# 2. Set up the simulation\n",
    "sample_size = 50\n",
    "n_samples = 1000\n",
    "sample_means = []\n",
    "\n",
    "# 3. Repeatedly sample and calculate the mean\n",
    "for _ in range(n_samples):\n",
    "    sample = np.random.choice(population, size=sample_size, replace=True)\n",
    "    sample_means.append(np.mean(sample))\n",
    "\n",
    "# 4. Plot the two distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot the population distribution\n",
    "sns.histplot(population, kde=True, ax=axes[0])\n",
    "axes[0].set_title(f\"Population Distribution (Skewed)\\nMean={np.mean(population):.2f}\")\n",
    "\n",
    "# Plot the sampling distribution of the mean\n",
    "sns.histplot(sample_means, kde=True, ax=axes[1])\n",
    "axes[1].set_title(f\"Sampling Distribution of the Mean (n={sample_size})\\nMean={np.mean(sample_means):.2f}\")\n",
    "\n",
    "plt.show()\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
